
---------
需要寫作，一律使用 Claude (Sonnet) 或 ChatGPT (GPT-4o)
需要推理，一律使用 GPT-o1 或 DeepSeek Chat (深度思考)
需要翻譯，一律選擇 Google AI Studio (長文) 或 沉浸式翻譯
需要事實，一律選擇 Felo Search
需要總結，一律選擇 Elmo 或 NotebookLLM
需要程式，一律選擇 GitHub Copilot 或 Mistral Chat
需要網頁，一律選擇 v0.dev 或 Lovable
需要配圖，一律選擇 Napkin (概念圖) 或 Eraser (架構圖)
需要生圖，一律選擇 DALL-E 3 或 ImageFx 或 Grok 2
需要音樂，一律選擇 Suno V4 或 StockTune
需要影片，一律選擇 Sora 或 PixVerse
需要口述，一律選擇 ElevenLabs
需要OCR，一律選擇 GPT-4o
需要字幕，一律選擇 Whisper (本機)
需要提示，一律選擇 Anthropic Console
需要速度，一律選擇 Groq 或 Cerebras
需要代理，一律選擇 Coze ( AI Agents )

---------
Anthropic 2025/5/22 開發者大會
https://www.youtube.com/playlist?list=PLf2m23nhTg1P5BsOHUOXyQz5RhfUSSVUi

---------

---------
從 Prompting 基本結構到 Agent Prompting 設計原則
https://ihower.tw/blog/13093-agent-prompting-design
Prompting 101
這一場用了一個實際的案例來講解：汽車保險理賠的自動化處理。

Prompt Structure 基礎 5 個結構元素:
1-2 句話建立角色和高層級任務描述 – 簡潔地告訴 AI 它的身份和主要目標
動態/檢索內容 – 放入需要處理的動態資料，如使用者偏好、地點資訊等
詳細任務指令 – 具體說明要如何執行任務的步驟和規則
範例/n-shot (optional) – 提供輸入輸出的範例來引導模型
重複關鍵指令 – 對於特別長的 prompt，在最後重申最重要的指令

擴充到更細節的 10 點結構: (當需要處理更複雜的任務時，可以將 prompt 結構擴充到 10 個元素)
任務（Task context） – 更詳細的角色設定和背景說明
語調（Tone context） – 明確設定回應的語氣、風格和個性
背景資料、文件和圖片 – 所有需要參考的靜態資料
詳細任務描述與規則 – 完整的執行步驟、判斷標準和處理規則
範例（Examples） – 多個不同情境的輸入輸出範例
對話歷史（Conversation history） – 如果有先前的對話記錄需要參考
立即任務描述或請求 – 當前這一輪具體要處理的任務
逐步思考提示 – 加入「Think step by step」或「Take a deep breath」等引導語
輸出格式要求（Output formatting） – 詳細指定回應的結構、格式和樣式
預填回應（Prefilled response） – 預先填入部分回應內容來引導模型 (編按: 這是 Claude API 才有的功能，可以預填 Assistant 回應。OpenAI API 沒有這功能)

講者最後提到 Extended Thinking 的用途: 何時用? 有沒有缺點?
Extended Thinking 可以用來輔助你的 system prompt 開發。在開發初期使用這個功能，觀察模型的推理過程是怎麼想的，了解模型的思考方式有助於你優化 prompt。
Extended Thinking 的缺點: 因為每次都重新發明輪子，因此花費 tokens 成本較高；另外因為溫度固定是 1 不能調整，所以比較不容易重現相同的結果。

Prompting for Agents
第二場演講則進入了不同的場景: 針對 Agent 系統的 prompting 設計，也就是 Agent 的 system prompt 要怎麼寫。
Agent更為複雜, 需先評估是否要用Agent
check list:
1. 任務是否複雜?
2. 是否有價值?
3. 是否可行? 若不可行需要再把scope拆成小一點
4. error cost是否很高? 高的話需使用read only, 並且在關鍵步驟加入人類的判斷

Agent Prompting 的 7 個核心原則：
1.像你的 Agent 一樣思考
設身處地思考 Agent 在執行時的 context 是否足夠
2.給予合理的啟發式方法
灌輸給模型需要的概念和一般性原則，避免模型面對 edge case 時走極端
3.清楚表達何時結束工具呼叫、何時達成目標
工具選擇是關鍵
讓模型知道任務要用哪些工具，什麼時候什麼情境要用
4.引導思考過程
預設開啟 extended thinking 的確效果就很好，但若能把 prompt 寫好引導可以表現更好
Claude 在呼叫工具之間也可以交錯思考
5.改變會有意想不到的副作用，要有心理準備
Agent 更難預測，例如只給「持續搜尋直到找到正確答案」可能造成模型用光 context window
需要告訴模型找不到完美來源也沒關係，可以在呼叫 3 次後就停止
6.需要協助 Agent 管理 context window
這有很多策略，例如 Claude Code 約在 190k tokens 時會自動壓縮成摘要、寫入外部檔案、用 sub-agent 處理後再回傳重要內容
7.讓 Claude 做 Claude
Claude 本身就擅長當 Agent，先用簡單 prompt 就好，不需要假設模型做不到

評估的重要性與挑戰
評估 Agent 比評估單一 Prompt 更困難，講者的建議是:
1.只要改變的影響效果很明顯，就不需要太多樣本。剛開始時只需要幾個測試案例，不需要很多 dataset 才能開始評估。
2.使用真實任務來反映現實狀況進行測試，評估你的研究系統時要用真實使用者可能會用的任務，理想上要有明確的正確答案，且可以使用現有工具找到。
3.配合評分標準的 LLM 評審非常強大，現在的 LLM 已經強大到能成為優秀的輸出評審，只要給予與人類判斷一致的明確評分標準即可。
4.但沒有任何東西能完美取代人工評估，沒有什麼比實際測試和體感檢查更好，與真實使用者測試時，人類最能找出系統的粗糙邊緣。

Agent Prompt 的建議結構範例
1. 任務描述
   - "You are a research assistant that searches the web"
2. 核心概念和原則（非詳細步驟）
   - 不可逆性原則
   - 工具選擇準則
   - 效率考量
3. 思考過程指引
   - "Plan your search process first"
   - "Reflect on the quality of results"
4. 邊界條件和限制
   - 預算限制（工具呼叫次數）
   - 停止條件
   - 錯誤處理
5. 上下文管理策略（如需要）
   - 何時壓縮記憶
   - 如何使用外部儲存
   
----------
https://www.ptt.cc/bbs/Soft_Job/M.1756208616.A.A0E.html
我不贊同AI開發時所謂"規格"驅動開發，也就是先寫完整套規格，再請AI開發的模式
理由主要有兩個：
1. AI內部的接力機制，假如一個AI 99%正確，迭代22次後會變成80%
2. 即使AI能夠連續工作N小時最後完成整套規格，但若是涉及UI/UX相關的，會有許多
   需要手動測試的地方   
# 個人方法
1. 要求AI建立單元與整合測試，UI/UX方面的手動測試則由我負責
2. 針對epic任務，讓AI自行建立文件紀錄說明與更新進度
3. 以我不親自更改程式碼為首要原則
4. 每次任務結束時都需要確保測試完全通過，並且通過我手動測試功能
5. 每一個階段都要進行lint與移除程式碼中的legacy code

一般迭代的流程如下：
POC -> 逐步增加feature -> MVP -> feature的增減與規格變更 -> 實作
POC到MVP這段，是目前AI的強項，特別是POC；沙士比亞後再無新故事，軟體開發方
面其實99%的人都在做重複的事，只是不同的數據、不同的組合而已
所以AI對我來說，能夠非常快看到可以操作的POC，並且快速迭代到MVP

但在MVP後，軟體需要進入打磨與增減的階段，這時AI的問題就特別明顯
1. AI在每個Task結束後對他來說又是新的開始，所以之前的細節他不一定會記得
   這導致AI的產出經常缺乏一致性，或是AI的注意力放在新功能的實現而忘記codebase
   可能存在相應的模組可以承擔部分職責，解決方法：
   a. 新增記憶 (但記憶過多無效)
   b. 你必須主動提示AI必須參考哪些module或文件
2. AI會把達成任務放在首要需求，意味著AI會不擇手段並選擇阻力最小的途徑來
   完成你的指示。這導致許多class過於臃腫，或function承擔了他原本不應該承
   擔的職責，甚至是不停累積的legacy程式碼。而這些legacy的程式碼又會成為干
   擾AI上下文的主因之一。
   解決方法：
   a. 指定AI建立特定的class並說明其職責
   b. 在指示階段直接指導重構的方法
3. 靜默錯誤。 AI會為了確保程式碼能夠"成功"執行而非"正確"執行，經常會過度
   地使用預設值、hardcode的數值作為回傳值，這導致一些層層包裝的模組在傳遞
   數值時又會過度累積大量的hardcode數值，使得底層模組出現真正異常時往往無
   法察覺
  
我看到網路上有些文章，確實會將設計模式、程式碼的設計準則一併加入prompt中，但
這仍然是一種關注程式碼結構與正確性的行為之一。所以，vibe coding目前「忘記程式
碼的存在」、「不用再管程式碼的結構與正確性」這樣的體驗至少我自己還無法感受到

推 qqqlll666: Llm目前就是當外部顧問 臨時工用 他沒辦法了解專案或 08/27 05:32
→ qqqlll666: 團隊文化等等隱性知識 就像請一個外部技術專家來 也不 08/27 05:32
→ qqqlll666: 可能三言兩語就上手完成任務 所以別期待AI可以幫你完 08/27 05:32
→ qqqlll666: 成一切 08/27 05:32
→ qqqlll666: 理想情況是在請他做事時一邊建立文件 建立測試 把隱形 08/27 05:32
→ qqqlll666: 知識轉成顯性 但這就像帶新人一樣 短期沒回報 長期也 08/27 05:32
→ qqqlll666: 不一定利己 最後說不定是讓自己失業
